# ui/batch_ui.py

import streamlit as st
from llm.mcqa_generator import download_new_batch_llm_mcqa
from ui.batch_components import (
    render_question_card,
    render_feedback_block,
    process_submission_batch
)

def render_batch_interface(llm_server):
    """
    Display the current batch in a form, handle submissions
    (including LLMâ€‘driven refinements), then immediately rerun
    so the UI clears or shows refined items, and finally
    prefetch the next batch in the background.
    """
    # â”€â”€ 1) Setup notice & prefetch flag â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if "upload_notice" not in st.session_state:
        st.session_state.upload_notice = (
            "ğŸš€ Your answers are being saved and uploaded in the background."
        )
    if "is_prefetching" not in st.session_state:
        st.session_state.is_prefetching = False

    st.markdown(f"ğŸš€ {st.session_state.upload_notice}")

    # â”€â”€ 2) Helper to prefetch next batch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def silent_prefetch():
        st.session_state.is_prefetching = True
        st.session_state.prefetched_batch = download_new_batch_llm_mcqa(
            llm_server=llm_server
        )
        st.session_state.is_prefetching = False

    # â”€â”€ 3) Render form for the current batch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.form("batch_form"):
        for i, record in enumerate(st.session_state.current_batch):
            render_question_card(record, i)
            render_feedback_block(record, i)

        submit = st.form_submit_button(
            "Submit All Answers",
            disabled=st.session_state.is_prefetching,
            help=(
                "â³ Still downloading the next batchâ€¦"
                if st.session_state.is_prefetching else None
            )
        )

    # â”€â”€ 4) On submit: process answers/refinements & mark submission â”€â”€
    if submit:
        with st.spinner("Processing submissionsâ€¦"):
            process_submission_batch(st.session_state.current_batch, llm_server)
            st.session_state.feedback_reset_counter += 1

        # Mark that we just submitted, so main() can clear the UI
        st.session_state.just_submitted = True

        # Force an immediate rerun so:
        #  â€¢ if current_batch is now empty â†’ you see only the intro + LLM closed Q&A
        #  â€¢ if some remain (had feedback) â†’ you see their updated questions/options
        st.experimental_rerun()

    # â”€â”€ 5) Background prefetch (only when a batch is displayed) â”€â”€â”€â”€â”€
    if (
        st.session_state.dataset_source != "Local Dataset"
        and not st.session_state.prefetched_batch
        and not st.session_state.is_prefetching
    ):
        silent_prefetch()
